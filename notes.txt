# PROJECT SETUP
cd /home/jhanak/Learning/vol_poc
export PROJECT_ROOT=$(pwd)
envsubst < .env | tee .env.evaluated


# ALIASES
alias psa='docker ps -a'
alias rma='docker rm -f $(docker ps -aq)'

# FUNCTIONS
# print networks and volume
function nv(){
  docker volume ls
  echo ""
  docker network ls
}

# Print all
function all(){
  echo ""
  docker ps -a
  echo ""
  nv
}

# Reset PG
function reset_pg(){
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml down postgres
  sudo rm -rf ./data/postgres_data && mkdir -p ./data/postgres_data && sudo chmod -R 777 ./data/postgres_data
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml up -d postgres
}

# Clean Setup
function clean_all(){
  # Clean up Hive - HMS & HS2
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml down -v

  # Clean up Hive - HMS & HS2
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml down -v
  rm -rf ${PROJECT_ROOT}/postgres_data && mkdir -p ${PROJECT_ROOT}/postgres_data && chmod -R 777 ${PROJECT_ROOT}/postgres_data
  rm -rf ${PROJECT_ROOT}/postgres_alt  && mkdir -p ${PROJECT_ROOT}/postgres_alt  && chmod -R 777 ${PROJECT_ROOT}/postgres_alt
  
  # Clean up MinIO
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-storage.yml down -v
  rm -rf ${PROJECT_ROOT}/minio_data

  # Clean up Base
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-base.yml down -v

  # Clean up Docker Volumes
  docker volume rm -f hive_data minio_data postgres_data postgres_alt

  # Clean up Docker Network
  docker volume rm -f data_platform_network

  # Print all
  all
}

# Start Setup
function start_all(){
  # Start up Base
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-base.yml up -d

  # Start up MinIO
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-storage.yml up -d

  # Start up Hive - HMS & HS2
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml up -d hive-metastore

  # Start up Spark Services
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml up -d spark-master spark-worker-1 spark-worker-2

  # Print all 
  all 
}

# Installing netcat and ss on HMS to check connectivity
docker exec -it -u root hive-metastore bash 
apt update && apt install iproute2 -y && apt install netcat
ss -ltnp | grep 10000
nc -zv hive-metastore 9083

# Installing delta-spark dependency for pyspark 
pip install delta-spark==2.2.0

# Creating a table on S3 bucket - upload the data file at S3 bucket manually via UI and followed by
docker exec -it -u root hive-metastore bash 
-- Create hive table
CREATE TABLE default.sample(
  id INTEGER,
  code STRING,
  name STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","
LINES TERMINATED BY  "\n"
LOCATION "s3a://raw-data/sample_data";

set hive.cli.print.header=true;
hive>
    >
    > SELECT * FROM sample a;
OK
a.id    a.code  a.name
1       A       Foo
2       B       Bar
Time taken: 0.094 seconds, Fetched: 2 row(s)

# Connection refused for HS2 - NOT pursuing in this setup
https://stackoverflow.com/questions/68985195/could-not-open-connection-to-the-hs2-server

# SPARK TESTING 
# #############
# Test Spark-Hive integration
docker rm -f spark-test && docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml up -d spark-test && docker logs -f spark-test

# Spark-Deltalake-HMS-MinIO Integration - Hive doesn't work with Delta
docker rm -f delta-lake-test && docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml up -d delta-lake-test && docker logs -f delta-lake-test

# Run this on spark-master to test tables from HMS and DeltaLake
docker exec -it -u root spark-master bash
pip install delta-spark==2.2.0
x='
from pyspark.sql import SparkSession
from delta.tables import DeltaTable
import os

def main():
  """
  Sample Delta Lake job that demonstrates:
  1. Creating a Delta table
  2. Performing ACID transactions (insert, update, delete)
  3. Time-travel capabilities
  4. Registering the Delta table in the Hive Metastore
  """
  # Initialize Spark session with Delta Lake support
  spark = SparkSession.builder \
    .appName("Delta Lake Demo") \
    .config("spark.jars.ivy","/tmp/.ivy") \
    .config("spark.sql.warehouse.dir", "s3a://delta-lake/warehouse") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .enableHiveSupport() \
    .getOrCreate()
  
  spark.sql("SHOW TABLES").show(truncate=False)
  spark.sql("SELECT * FROM sample_sales").show(truncate=False)
  spark.sql("SELECT * FROM delta_products").show(truncate=False)
  spark.stop()
    
if __name__ == "__main__":
  main()
'

echo -e "${x}" > test.py

/opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=/tmp/.ivy \
  --conf spark.sql.catalogImplementation=hive \
  --conf "spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083" \
  --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
  --conf "spark.hadoop.fs.s3a.access.key=minioadmin" \
  --conf "spark.hadoop.fs.s3a.secret.key=minioadmin" \
  --conf "spark.hadoop.fs.s3a.path.style.access=true" \
  --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
  --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.logger.org.apache=WARN" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j.logger.org.apache=WARN" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.rootCategory=WARN,console" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties" \
  --jars /opt/bitnami/spark/jars/delta-core_2.12-2.2.0,/opt/bitnami/spark/jars/delta-storage-2.2.0.jar \
  /opt/bitnami/spark/test.py
  
# Trying pyspark shell to check tables 
root@c0bada8b642f:/opt/bitnami/spark# /opt/bitnami/spark/bin/pyspark \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=/tmp/.ivy \
  --conf spark.sql.catalogImplementation=hive \
  --conf "spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083" \
  --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
  --conf "spark.hadoop.fs.s3a.access.key=minioadmin" \
  --conf "spark.hadoop.fs.s3a.secret.key=minioadmin" \
  --conf "spark.hadoop.fs.s3a.path.style.access=true" \
  --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
  --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.logger.org.apache=WARN" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j.logger.org.apache=WARN" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.rootCategory=WARN,console" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties" \
  --jars /opt/bitnami/spark/jars/delta-core_2.12-2.2.0,/opt/bitnami/spark/jars/delta-storage-2.2.0.jar
Python 3.8.15 (default, Oct 25 2022, 21:42:05) 
[GCC 10.2.1 20210110] on linux
Type "help", "copyright", "credits" or "license" for more information.
ERROR StatusLogger Reconfiguration failed: No configuration found for '59f95c5d' at 'null' in 'null'
ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'
25/04/09 06:51:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/

Using Python version 3.8.15 (default, Oct 25 2022 21:42:05)
Spark context Web UI available at http://c0bada8b642f:4040
Spark context available as 'sc' (master = spark://spark-master:7077, app id = app-20250409065126-0008).
SparkSession available as 'spark'.
>>> 
>>> spark.sql("SHOW TABLES").show(truncate=False)
2025-04-09 06:51:47,039 Thread-5 ERROR Reconfiguration failed: No configuration found for '68665119' at 'null' in 'null'
+---------+--------------+-----------+                                          
|namespace|tableName     |isTemporary|
+---------+--------------+-----------+
|default  |delta_products|false      |
|default  |sample        |false      |
|default  |sample2       |false      |
|default  |sample_sales  |false      |
+---------+--------------+-----------+

>>> 
>>> spark.sql("SELECT * FROM delta_products").show(truncate=False)
25/04/09 06:52:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
25/04/09 06:52:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
+---+------------+------+----------+                                            
|id |product_name|price |sale_date |
+---+------------+------+----------+
|7  |Product G   |175.25|2023-01-21|
|6  |Product F   |250.0 |2023-01-20|
|3  |Product C   |200.0 |2023-01-17|
|4  |Product D   |300.0 |2023-01-18|
|1  |Product A   |100.5 |2023-01-15|
|2  |Product B   |200.75|2023-01-16|
+---+------------+------+----------+
