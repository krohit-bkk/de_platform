# PROJECT SETUP
cd /home/jhanak/Learning/vol_poc
export PROJECT_ROOT=$(pwd)
envsubst < .env | tee .env.evaluated


# ALIASES
alias psa='docker ps -a'
alias rma='docker rm -f $(docker ps -aq)'

# FUNCTIONS
# print networks and volume
function nv(){
  docker volume ls
  echo ""
  docker network ls
}

# Print all
function all(){
  echo ""
  docker ps -a
  echo ""
  nv
}

# Reset PG
function reset_pg(){
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml down postgres
  sudo rm -rf ./data/postgres_data && mkdir -p ./data/postgres_data && sudo chmod -R 777 ./data/postgres_data
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml up -d postgres
}

# Clean Setup
function clean_all(){
  # Clean up Hive - HMS & HS2
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml down -v

  # Clean up Hive - HMS & HS2
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml down -v
  rm -rf ${PROJECT_ROOT}/postgres_data && mkdir -p ${PROJECT_ROOT}/postgres_data && chmod -R 777 ${PROJECT_ROOT}/postgres_data
  rm -rf ${PROJECT_ROOT}/postgres_alt  && mkdir -p ${PROJECT_ROOT}/postgres_alt  && chmod -R 777 ${PROJECT_ROOT}/postgres_alt
  
  # Clean up MinIO
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-storage.yml down -v
  rm -rf ${PROJECT_ROOT}/minio_data

  # Clean up Base
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-base.yml down -v

  # Clean up Docker Volumes
  docker volume rm -f hive_data minio_data postgres_data postgres_alt

  # Clean up Docker Network
  docker volume rm -f data_platform_network

  # Print all
  all
}

# Start Setup
function start_all(){
  # Start up Base
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-base.yml up -d

  # Start up MinIO
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-storage.yml up -d

  # Start up Hive - HMS & HS2
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-metastore.yml up -d hive-metastore

  # Start up Spark Services
  docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml up -d spark-master spark-worker-1 spark-worker-2

  # Print all 
  all 
}

# Installing netcat and ss on HMS to check connectivity
docker exec -it -u root hive-metastore bash 
apt update && apt install iproute2 -y && apt install netcat
ss -ltnp | grep 10000
nc -zv hive-metastore 9083

# Installing delta-spark dependency for pyspark 
pip install delta-spark==2.2.0

# Creating a table on S3 bucket - upload the data file at S3 bucket manually via UI and followed by
docker exec -it -u root hive-metastore bash 
-- Create hive table
CREATE TABLE default.sample(
  id INTEGER,
  code STRING,
  name STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","
LINES TERMINATED BY  "\n"
LOCATION "s3a://raw-data/sample_data";

set hive.cli.print.header=true;
hive>
    >
    > SELECT * FROM sample a;
OK
a.id    a.code  a.name
1       A       Foo
2       B       Bar
Time taken: 0.094 seconds, Fetched: 2 row(s)

# Connection refused for HS2 - NOT pursuing in this setup
https://stackoverflow.com/questions/68985195/could-not-open-connection-to-the-hs2-server

# Test Spark-Hive integration
docker rm -f spark-test && docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml up -d spark-test && docker logs -f spark-test

# Spark-Deltalake-HMS-MinIO Integration - Hive doesn't work with Delta
docker rm -f delta-lake-test && docker-compose --env-file .env.evaluated -f ./docker-compose/docker-compose-processing.yml up -d delta-lake-test && docker logs -f delta-lake-test

# Run this on spark-master to test tables from HMS and DeltaLake
docker exec -it -u root spark-master bash
pip install delta-spark==2.2.0
x='
from pyspark.sql import SparkSession
from delta.tables import DeltaTable
import os

def main():
    """
    Sample Delta Lake job that demonstrates:
    1. Creating a Delta table
    2. Performing ACID transactions (insert, update, delete)
    3. Time-travel capabilities
    4. Registering the Delta table in the Hive Metastore
    """
    # Initialize Spark session with Delta Lake support
    spark = SparkSession.builder \
        .appName("Delta Lake Demo") \
        .config("spark.jars.ivy","/tmp/.ivy") \
        .config("spark.sql.warehouse.dir", "s3a://delta-lake/warehouse") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .enableHiveSupport() \
        .getOrCreate()
   
    spark.sql("SHOW TABLES").show(truncate=False)
    spark.sql("SELECT * FROM sample_sales").show(truncate=False)
    spark.sql("SELECT * FROM delta_products").show(truncate=False)
    spark.stop()
    
if __name__ == "__main__":
    main()
'

echo -e "${x}" > test.py

/opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=/tmp/.ivy \
  --conf spark.sql.catalogImplementation=hive \
  --conf "spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083" \
  --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
  --conf "spark.hadoop.fs.s3a.access.key=minioadmin" \
  --conf "spark.hadoop.fs.s3a.secret.key=minioadmin" \
  --conf "spark.hadoop.fs.s3a.path.style.access=true" \
  --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
  --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.logger.org.apache=WARN" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j.logger.org.apache=WARN" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.rootCategory=WARN,console" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties" \
  --jars /opt/bitnami/spark/jars/delta-core_2.12-2.2.0,/opt/bitnami/spark/jars/delta-storage-2.2.0.jar \
  /opt/bitnami/spark/test.py
  